{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasecondac99415fb886847e59d8e5aaf8efeb2dd",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning For NLP Using Pretrained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.layers import Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download imdb dataset from keras\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\Sam Cannon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 20, 8)             80000     \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 160)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 161       \n=================================================================\nTotal params: 80,161\nTrainable params: 80,161\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:From C:\\Users\\Sam Cannon\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nTrain on 20000 samples, validate on 5000 samples\nEpoch 1/10\n20000/20000 [==============================] - 1s 61us/step - loss: 0.6799 - acc: 0.5933 - val_loss: 0.6427 - val_acc: 0.6798\nEpoch 2/10\n20000/20000 [==============================] - 1s 55us/step - loss: 0.5611 - acc: 0.7452 - val_loss: 0.5343 - val_acc: 0.7230\nEpoch 3/10\n20000/20000 [==============================] - 1s 52us/step - loss: 0.4682 - acc: 0.7842 - val_loss: 0.5021 - val_acc: 0.7440\nEpoch 4/10\n20000/20000 [==============================] - 1s 55us/step - loss: 0.4252 - acc: 0.8075 - val_loss: 0.4960 - val_acc: 0.7504\nEpoch 5/10\n20000/20000 [==============================] - 1s 60us/step - loss: 0.3972 - acc: 0.8220 - val_loss: 0.4976 - val_acc: 0.7538\nEpoch 6/10\n20000/20000 [==============================] - 1s 54us/step - loss: 0.3759 - acc: 0.8356 - val_loss: 0.4970 - val_acc: 0.7570\nEpoch 7/10\n20000/20000 [==============================] - 1s 52us/step - loss: 0.3564 - acc: 0.8465 - val_loss: 0.5022 - val_acc: 0.7544\nEpoch 8/10\n20000/20000 [==============================] - 1s 55us/step - loss: 0.3383 - acc: 0.8573 - val_loss: 0.5083 - val_acc: 0.7540\nEpoch 9/10\n20000/20000 [==============================] - 1s 51us/step - loss: 0.3216 - acc: 0.8661 - val_loss: 0.5167 - val_acc: 0.7494\nEpoch 10/10\n20000/20000 [==============================] - 1s 54us/step - loss: 0.3051 - acc: 0.8744 - val_loss: 0.5218 - val_acc: 0.7536\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length = maxlen))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Labels of Raw Text Data for IMDB Dataset\n",
    "- downloaded the zip file from http://mng.bz/0tIo\n",
    "- unzip files and then label the text based on neg or pos review\n",
    "- this will show how using pretrained word embeddings can greatly aid in nlp tasks that have only a small amount of training data (even though this isn't the case here, using more samples than 200 would help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imbdbdir = 'c:\\\\Users\\\\Sam Cannon\\\\Desktop\\\\Python\\\\Deep Learning With Python'\n",
    "train_dir = os.path.join(imbdbdir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding = 'utf8')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 88582 unique tokens.\nShape of data tensor: (25000, 100)\nShape of label tensor: (25000,)\n"
    }
   ],
   "source": [
    "#use only 200 reviews, using pretrained word embeddings\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print(f'Shape of data tensor: {data.shape}')\n",
    "print(f'Shape of label tensor: {labels.shape}')\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GloVe and FastText Word Embeddings\n",
    "- download wikipedia 2014 embeddings from https://nlp.stanford.edu/projectz/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 999995 word vectors.\n"
    }
   ],
   "source": [
    "# parsing fasttext word-embeddings (1,000,000 vectors) \n",
    "fasttext_dir = 'c:\\\\Users\\\\Sam Cannon\\\\Desktop\\\\Python\\\\fasttext'\n",
    "\n",
    "fasttext_index = {}\n",
    "f = open(os.path.join(fasttext_dir, 'wiki-news-300d-1M.vec'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    fasttext_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f'Found {len(fasttext_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors.\n"
    }
   ],
   "source": [
    "#parsing GlOve word vectors\n",
    "glove_dir = 'c:\\\\Users\\\\Sam Cannon\\\\Desktop\\\\Python\\\\Deep Learning With Python\\\\GLOVE'\n",
    "\n",
    "glove_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(glove_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = fasttext_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 100, 300)          3000000   \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 30000)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                960032    \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 3,960,065\nTrainable params: 3,960,065\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now load the GlOve matrix we prepared above into the Embedding layer, the first layer in the model\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False #freeze the Embedding layer, the pretrained word embeddings shouldn't be updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 200 samples, validate on 10000 samples\nEpoch 1/10\n200/200 [==============================] - 1s 6ms/step - loss: 0.9146 - acc: 0.4750 - val_loss: 0.6847 - val_acc: 0.5491\nEpoch 2/10\n200/200 [==============================] - 1s 5ms/step - loss: 0.2736 - acc: 0.9600 - val_loss: 0.7351 - val_acc: 0.5376\nEpoch 3/10\n200/200 [==============================] - 1s 6ms/step - loss: 0.1084 - acc: 0.9900 - val_loss: 0.9920 - val_acc: 0.5052\nEpoch 4/10\n200/200 [==============================] - 1s 6ms/step - loss: 0.0603 - acc: 1.0000 - val_loss: 0.9471 - val_acc: 0.5176\nEpoch 5/10\n200/200 [==============================] - 1s 5ms/step - loss: 0.0228 - acc: 1.0000 - val_loss: 0.7766 - val_acc: 0.5644\nEpoch 6/10\n200/200 [==============================] - 1s 5ms/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.7770 - val_acc: 0.5584\nEpoch 7/10\n200/200 [==============================] - 1s 5ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.7329 - val_acc: 0.5811\nEpoch 8/10\n200/200 [==============================] - 1s 5ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.7327 - val_acc: 0.5864\nEpoch 9/10\n200/200 [==============================] - 1s 5ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.7899 - val_acc: 0.5650\nEpoch 10/10\n200/200 [==============================] - 1s 5ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.7508 - val_acc: 0.5867\n"
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on test set, first we must unpack and label all the test text\n",
    "test_dir = os.path.join('c:\\\\Users\\\\Sam Cannon\\\\Desktop\\\\Python\\\\Deep Learning With Python', 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    " \n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding = 'utf8')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen = maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "20007/20007 [==============================] - 2s 118us/step\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.7882445617603828, 0.5636526942253113]"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "#now load and evaluate the model on the test set -- terrible accuracy, this is becuase we are only looking at 200 samples!\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}